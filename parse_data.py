from __future__ import print_function

import csv
import io
import math
import numpy as np
import var
import imp
import sys
sys.modules["sqlite"] = imp.new_module("sqlite")
sys.modules["sqlite3.dbapi2"] = imp.new_module("sqlite.dbapi2")
import nltk

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer, word_tokenize


def get_glove_vectors(path):
    '''
    Given a glove embeddings txt file, parse the file into a dictionary
    where every key in the dictionary is a word and the value is the word
    vector associated with the word.

    Inputs:
      path: path to glove embeddings file

    Outputs:
      embedding dictionary where every key is a word and maps to an
      embedding vector
    '''
    embeddings = {}
    with open(path, 'r') as f:
        for line in f:
            data = line.strip().split()
            word = data[0]
            vector = np.asarray(data[1:], dtype='float32')
            embeddings[word] = vector
    return embeddings


def get_input_data(path):
    '''
    Converts the train, eval, and test TSVs into arrays in memory that
    can be easily accessed.

    # Description of the TSV format:
    #
    # Column 1: the ID of the statement ([ID].json).
    # Column 2: the label.
    # Column 3: the statement.
    # Column 4: the subject(s).
    # Column 5: the speaker.
    # Column 6: the speaker's job title.
    # Column 7: the state info.
    # Column 8: the party affiliation.
    # Column 9-13: the total credit history count, including the
                    current statement.
    # 9: barely true counts.
    # 10: false counts.
    # 11: half true counts.
    # 12: mostly true counts.
    # 13: pants on fire counts.
    # Column 14: the context (venue / location of the speech or
                    statement).

    Inputs:
      path: path to TSV file to parse
    Outputs:
      data: 2d list representation of data
    '''
    with open(path, 'r', newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter="\t", quotechar='"')
        data = [[index for index in row] for row in reader]
    return data


def get_data(path):
    '''
    From the 2d list representation of the data TSVs generated by
    get_input_data, return lists representing the labels, sentences,
    subjects, party, and history for each data point.

    Inputs:
      path: path to TSV file to get data from
    Outputs:
      labels: list of labels for each data point
      sentences: list of sentences for each data point
      subjects: list of subjects for each data point
      party: list of parties for each data point
      history: list of histories for each data point.
    '''
    data = get_input_data(path)
    labels = [datum[1] for datum in data]
    # remove non-ascii characters
    sentences = [''.join([i if ord(i) < 128 else ' ' for i in datum[2]])
                 for datum in data]
    subjects = [datum[3].split(',') for datum in data]
    party = [datum[7] for datum in data]
    history = [[int(datum[i]) for i in range(8, 13)] for datum in data]
    return labels, sentences, subjects, party, history


def get_mapping(list_of_list_of_items):
    '''
    Given a list of list of items, return a dictionary with the frequency of
    each item.

    Inputs:
      list_of_list_of_items: list of list of items to get the frequency of
    Outputs:
      mapping: dictionary mapping items to their frequency
    '''
    mapping = {}
    for list_of_items in list_of_list_of_items:
        for item in list_of_items:
            if item not in mapping:
                mapping[item] = 0
            mapping[item] += 1
    return mapping


def get_one_hot_vectors(list_of_values, length, mapping):
    '''
    Create one hot vectors from a list of values given a frequency
    mapping of words and expected length of the one hot vector. First, the
    words used by the one hot vector are determined by selecting 'length'
    words of highest frequency. Then, convert each value in the list of
    values to one hot vectors.

    Inputs:
      list_of_values: list of values to convert into one hot vectors
      length: length of one hot vectors
      mapping: frequency mapping of words in vocabulary
    Outputs:
      vectors: list of one hot values corresponding to the original list of
      values
    '''
    top_values = sorted(
        mapping.items(),
        key=lambda x: x[1],
        reverse=True)[
        :length]
    one_hot_values = {top_values[i][0]: i for i in range(len(top_values))}

    vectors = []
    for values in list_of_values:
        vector = [0] * length
        for value in values:
            if value in one_hot_values:
                index = one_hot_values[value]
                vector[index] = 1
        vectors.append(vector)
    return vectors


def normalize_vectors(list_of_values):
    '''
    Normalize a list of vectors such that each vector has magnitude 1.

    Inputs:
      list_of_values: list of vectors to normalize
    Outputs:
      normalized: list of normalized vectors corresponding to the input list
                  of values.
    '''
    normalized = []
    for values in list_of_values:
        sum = 0.0
        for value in values:
            sum += value
        # zero sum case
        if sum == 0:
            sum = 1
        normalized.append([value / sum for value in values])
    return normalized


def clean_credit(labels, credit):
    '''
    Removes from the credit history the current label. It's necessary to do
    this before using the credit metadata as mentioned in the original paper;
    otherwise, there are many credit histories with only 1 data point, which
    the credit metadata will directly map to.

    Inputs:
      labels: list of labels for each data point
      credit: list of credits for each data point
    Outputs:
      credit: list of credits with label of data point removed
    '''
    # remove from credit vector the current label
    for i in range(len(labels)):
        if labels[i] in var.CREDIT_MAPPING:
            remove_index = var.CREDIT_MAPPING[labels[i]]
            credit[i][remove_index] -= 1
    return credit


def get_pos_freqs(sentences):
    '''
    Gets the part of speech frequency from each sentence using nltk pos_tag.

    Inputs:
      sentences: list of sentences to get part of speech frequencies
    Outputs:
      vector_list: returns a list of list of pos frequencies where each list of
                   pos frequencies corresponds to an input sentence
    '''
    vector_list = []
    for sentence in sentences:
        tokens = nltk.tokenize.word_tokenize(sentence)
        tagged = nltk.pos_tag(tokens)
        vector = [0] * var.POS_TAG_SET_LENGTH
        for tag in tagged:
            vector[var.POS_TAG_SET[tag[1]]] += 1
        vector_list.append(vector)
    return vector_list


def remove_stop_words(sentences):
    '''
    Removes stopwords from a sentence using nltk.

    Inputs:
      sentences: list of sentences to remove stopwords from
    Outputs:
      sentences: list of sentences with stopwords removed
    '''
    stops = set(stopwords.words('english'))
    sentences = [word_tokenize(sentence) for sentence in sentences]
    sentences = [[word for word in sentence if word not in stops]
                 for sentence in sentences]
    sentences = [' '.join(word for word in sentence) for sentence in sentences]
    return sentences
